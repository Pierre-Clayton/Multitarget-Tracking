{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCMC-PF Filtering:  57%|█████▋    | 57/100 [07:22<05:19,  7.43s/it]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MCMC-Based Particle Filter for Multi-Target Tracking\n",
    "====================================================\n",
    "\n",
    "This script demonstrates:\n",
    " 1) Ground-truth generation for up to 5 targets (with births/deaths).\n",
    " 2) Poisson clutter + target-origin measurements.\n",
    " 3) An MCMC-based particle filter to estimate the (x, y) positions and (vx, vy) velocities\n",
    "    of each target, and whether each target exists or not at each time step.\n",
    " 4) Plots:\n",
    "    - True tracks vs. estimated tracks (best-weight particle) in x-y plane.\n",
    "    - Cardinality (true vs. estimated).\n",
    " 5) A progress bar (via tqdm) to indicate the filtering progress.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import rand, randn, choice\n",
    "from scipy.stats import poisson, multivariate_normal\n",
    "from tqdm import tqdm  # for progress bar\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1) GROUND-TRUTH GENERATION\n",
    "# ----------------------------------------------------------------------\n",
    "def generate_ground_truth(K=80,  # total number of timesteps\n",
    "                          tau=3.0,\n",
    "                          sigma_process=0.5,\n",
    "                          scenario_params=None):\n",
    "    \"\"\"\n",
    "    Generate ground-truth states for up to 5 targets, with known birth times\n",
    "    and at least one known death time. Targets follow a near-constant velocity (NCV) model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    K : int\n",
    "        Number of timesteps.\n",
    "    tau : float\n",
    "        Sampling interval (seconds).\n",
    "    sigma_process : float\n",
    "        Standard deviation of the process noise for NCV model.\n",
    "    scenario_params : dict\n",
    "        Dictionary specifying each target's birth_time, death_time, and initial state.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ground_truth : dict\n",
    "        Contains:\n",
    "          x_true : array, shape=(K, Nmax, 4), the true states for each target\n",
    "                   (x, vx, y, vy).\n",
    "          e_true : array, shape=(K, Nmax), the existence indicators (0 or 1).\n",
    "    \"\"\"\n",
    "\n",
    "    # Default scenario: 5 targets\n",
    "    #   T1, T2, T3 born at k=1\n",
    "    #   T4, T5 born at k=25\n",
    "    #   T1 dies at k=50\n",
    "    if scenario_params is None:\n",
    "        scenario_params = {\n",
    "            0: {'birth_time': 1,\n",
    "                'death_time': 50,\n",
    "                'init_state': np.array([500.0,   1.0,  500.0,   15.0])},  \n",
    "            1: {'birth_time': 1,\n",
    "                'death_time': None,\n",
    "                'init_state': np.array([1000.0, -1.0, 4000.0, -10.0])},\n",
    "            2: {'birth_time': 1,\n",
    "                'death_time': None,\n",
    "                'init_state': np.array([4000.0,   5.0, 1000.0,   5.0])},\n",
    "            3: {'birth_time': 25,\n",
    "                'death_time': None,\n",
    "                'init_state': np.array([4500.0,  -5.0, 4500.0,   5.0])},\n",
    "            4: {'birth_time': 25,\n",
    "                'death_time': None,\n",
    "                'init_state': np.array([500.0,   10.0, 4500.0,  -15.0])}\n",
    "        }\n",
    "\n",
    "    Nmax = len(scenario_params)  # should be 5\n",
    "    x_true = np.zeros((K, Nmax, 4))\n",
    "    e_true = np.zeros((K, Nmax), dtype=int)\n",
    "\n",
    "    # Define the state transition matrix for near-constant velocity (NCV)\n",
    "    I_2 = np.array([[1, 0],\n",
    "                    [0, 1]], dtype=float)\n",
    "    tau_I_2 = np.array([[tau, 0],\n",
    "                        [0, tau]], dtype=float)\n",
    "    A = np.block([[I_2,           tau_I_2],\n",
    "                  [np.zeros((2, 2)), I_2          ]])\n",
    "    # Process noise covariance\n",
    "    q = sigma_process**2\n",
    "    Q = q * np.array([[tau**3/3, 0, tau**2/2,         0],\n",
    "                      [0,       tau**3/3, 0,         tau**2/2],\n",
    "                      [tau**2/2,        0,         tau, 0],\n",
    "                      [0,        tau**2/2,         0,       tau]])\n",
    "\n",
    "    # Initialize states for the first timestep\n",
    "    for n in range(Nmax):\n",
    "        bt = scenario_params[n]['birth_time']\n",
    "        dt = scenario_params[n]['death_time']\n",
    "        x0 = scenario_params[n]['init_state'].copy()\n",
    "\n",
    "        if bt is not None and 1 >= bt:\n",
    "            x_true[0, n, :] = x0\n",
    "            e_true[0, n] = 1\n",
    "\n",
    "    # Generate ground truth for subsequent timesteps\n",
    "    for k in range(1, K):\n",
    "        for n in range(Nmax):\n",
    "            bt = scenario_params[n]['birth_time']\n",
    "            dt = scenario_params[n]['death_time']\n",
    "\n",
    "            if bt is not None and k >= bt and (dt is None or k < dt):\n",
    "                # If k == bt, then the target is just born at this step\n",
    "                if k == bt:\n",
    "                    x_true[k, n, :] = scenario_params[n]['init_state']\n",
    "                    e_true[k, n] = 1\n",
    "                else:\n",
    "                    # Continue the NCV model\n",
    "                    x_prev = x_true[k-1, n, :]\n",
    "                    mean = A @ x_prev\n",
    "                    x_true[k, n, :] = np.random.multivariate_normal(mean, Q)\n",
    "                    e_true[k, n] = 1\n",
    "            else:\n",
    "                # Target does not exist at time k\n",
    "                x_true[k, n, :] = 0.0\n",
    "                e_true[k, n] = 0\n",
    "\n",
    "    return {'x_true': x_true, 'e_true': e_true}\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2) MEASUREMENT GENERATION\n",
    "# ----------------------------------------------------------------------\n",
    "def generate_measurements(ground_truth,\n",
    "                          Lx=5000.0, \n",
    "                          Ly=5000.0,\n",
    "                          LambdaC=20.0,  # Mean clutter rate (Poisson)\n",
    "                          LambdaX=1.0,   # Mean detection rate per active target (Poisson)\n",
    "                          Sigma_x=100.0, # Measurement variance (Sigma_x * I2)\n",
    "                          seed=None):\n",
    "    \"\"\"\n",
    "    Generate a set of measurements for each time step `k` based on the\n",
    "    'association-free' model described in the article.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ground_truth : dict\n",
    "        Dictionary containing at least:\n",
    "          - ground_truth['x_true']: array of shape (K, Nmax, 4),\n",
    "            where x_true[k, n, :] = (x_k,n, vx_k,n, y_k,n, vy_k,n).\n",
    "          - ground_truth['e_true']: array of shape (K, Nmax),\n",
    "            where e_true[k, n] = 1 if target `n` exists at time `k`, otherwise 0.\n",
    "    Lx, Ly : float\n",
    "        Dimensions of the surveillance region in meters.\n",
    "    LambdaC : float\n",
    "        Mean number of clutter (false alarms) per time step.\n",
    "    LambdaX : float\n",
    "        Mean number of detections per active target and per time step.\n",
    "    Sigma_x : float\n",
    "        Measurement variance in both x and y directions (covariance is Sigma_x * I2).\n",
    "    seed : int or None\n",
    "        Seed for the random number generator (for reproducibility).\n",
    "        If None, results will be random.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    measurements : list\n",
    "        A list of length K.\n",
    "        measurements[k] is a numpy array of shape (M_k, 2),\n",
    "        where M_k is the total number of measurements at time step `k` (target + clutter).\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)  # For optional reproducibility\n",
    "\n",
    "    x_true = ground_truth['x_true']  # (K, Nmax, 4)\n",
    "    e_true = ground_truth['e_true']  # (K, Nmax)\n",
    "    K, Nmax, _ = x_true.shape\n",
    "\n",
    "    # Measurement covariance matrix: Sigma_x * I2\n",
    "    R_meas = Sigma_x * np.eye(2)\n",
    "\n",
    "    # List to store measurements for each time step k\n",
    "    measurements = []\n",
    "\n",
    "    # Loop through each time step k\n",
    "    for k in range(K):\n",
    "        # -- 1) Identify active targets\n",
    "        active_targets = np.where(e_true[k, :] == 1)[0]\n",
    "\n",
    "        # -- 2) Generate target-originated measurements\n",
    "        #     For each active target, draw the number of measurements from Poisson(LambdaX).\n",
    "        target_meas_list = []\n",
    "        for n in active_targets:\n",
    "            # Number of measurements for target `n`\n",
    "            count_target_n = poisson.rvs(LambdaX)\n",
    "            if count_target_n > 0:\n",
    "                # Mean of the Gaussian distribution for target `n`: (x_k,n, y_k,n)\n",
    "                mean_xy = np.array([\n",
    "                    x_true[k, n, 0],  # x_k,n\n",
    "                    x_true[k, n, 2]   # y_k,n\n",
    "                ])\n",
    "                # Generate Gaussian measurements\n",
    "                z_n = np.random.multivariate_normal(mean_xy, R_meas, size=count_target_n)\n",
    "                target_meas_list.append(z_n)\n",
    "\n",
    "        # -- 3) Generate clutter (false alarms)\n",
    "        #     Draw the number of clutter points from Poisson(LambdaC).\n",
    "        clutter_count = poisson.rvs(LambdaC)\n",
    "        # Clutter measurements are uniformly distributed in [0, Lx] x [0, Ly].\n",
    "        clutter_meas = np.column_stack([\n",
    "            np.random.uniform(0, Lx, size=clutter_count),\n",
    "            np.random.uniform(0, Ly, size=clutter_count)\n",
    "        ])\n",
    "\n",
    "        # -- 4) Combine target and clutter measurements\n",
    "        if len(target_meas_list) > 0:\n",
    "            all_target_meas = np.vstack(target_meas_list)  # Concatenate all target measurements\n",
    "            all_meas = np.vstack([all_target_meas, clutter_meas])\n",
    "        else:\n",
    "            # If there are no active targets or Poisson(LambdaX)=0, only clutter measurements remain\n",
    "            all_meas = clutter_meas\n",
    "\n",
    "        measurements.append(all_meas)\n",
    "\n",
    "    return measurements\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3) MCMC-BASED PARTICLE FILTER CLASS\n",
    "# ----------------------------------------------------------------------\n",
    "class MCMCParticleFilter:\n",
    "    \"\"\"\n",
    "    MCMC-Based Particle Filter for a time-varying number of targets, as described\n",
    "    in Section III–IV of the article.\n",
    "\n",
    "    The targets are modeled with:\n",
    "      - Existence variables e_{k,n} in {0,1} (Eq. (6))\n",
    "      - Kinematics x_{k,n} = (x, vx, y, vy) (fixed dimension Nmax x 4)\n",
    "      - Transition densities:\n",
    "         * Birth:   Eq. (8)\n",
    "         * Death:   Eq. (9)\n",
    "         * Update:  Eq. (10)\n",
    "      - Association-free measurement model: Eqs. (12)–(13)\n",
    "      - MCMC approach: Eqs. (14)–(15) for refinement\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 Np=4000,\n",
    "                 Nburn=1000,\n",
    "                 thinning=6,\n",
    "                 PB=0.01,       # Probability of birth, Eq. (6)\n",
    "                 PD=0.01,       # Probability of death, Eq. (6)\n",
    "                 sigma_process=0.5,  # std. dev. squared in NCV model\n",
    "                 tau=3.0,       # sampling interval\n",
    "                 R=100.0,       # measurement noise covariance = 100 * I2\n",
    "                 LambdaC=20.0,  # clutter rate\n",
    "                 LambdaX=1.0,   # detection rate per active target\n",
    "                 Lx=5000.0,\n",
    "                 Ly=5000.0,\n",
    "                 x_death=None,\n",
    "                 Vmax=20.0):    # max speed for birth sampling\n",
    "        \"\"\"\n",
    "        Initialize MCMC-based Particle Filter with default parameters from the article:\n",
    "          - Np=4000 (num. particles)\n",
    "          - Nburn=1000 (burn-in for MCMC)\n",
    "          - thinning=6\n",
    "          - PB, PD in [0,1]\n",
    "          - sigma_process=0.5\n",
    "          - R=100 => CovMeas = 100 * I2\n",
    "          - LambdaC=20, LambdaX=1 => Poisson parameters for clutter/target measurements\n",
    "          - Lx=5000, Ly=5000 => surveillance area\n",
    "          - x_death: placeholder state for 'dead' target (Eq. (9))\n",
    "          - Vmax=20 => uniform speed range in [-Vmax, Vmax] at birth (Eq. (8)).\n",
    "        \"\"\"\n",
    "        self.Np = Np\n",
    "        self.Nburn = Nburn\n",
    "        self.thinning = thinning\n",
    "\n",
    "        # Eq. (6) parameters for birth/death processes\n",
    "        self.PB = PB  # Probability to become alive if previously dead\n",
    "        self.PD = PD  # Probability to become dead if previously alive\n",
    "\n",
    "        # NCV model parameters (Eq. (10) and (11))\n",
    "        self.sigma_process = sigma_process\n",
    "        self.tau = tau\n",
    "\n",
    "        # Observation model parameters\n",
    "        self.R = R\n",
    "        self.LambdaC = LambdaC\n",
    "        self.LambdaX = LambdaX\n",
    "\n",
    "        # Surveillance area and speeds\n",
    "        self.Lx = Lx\n",
    "        self.Ly = Ly\n",
    "        self.Vmax = Vmax\n",
    "\n",
    "        # \"Death\" state for inactive targets (Eq. (9))\n",
    "        # By default, use zero for all (x,y,vx,vy).\n",
    "        if x_death is None:\n",
    "            self.x_death = np.zeros(4, dtype=float)\n",
    "        else:\n",
    "            self.x_death = x_death\n",
    "\n",
    "        # Near-constant velocity (NCV) state transition matrix A_k,n (Eq. (11))\n",
    "        # A = [[1, 0, tau,   0  ],\n",
    "        #      [0,  1,   0,  tau ],\n",
    "        #      [0,  0,   1,  0],\n",
    "        #      [0,  0,   0,  1 ]]\n",
    "        tau_I2 = np.array([[self.tau, 0],\n",
    "                            [0, self.tau]], dtype=float)\n",
    "        I2 = np.array([[1, 0],\n",
    "                      [0, 1]], dtype=float)\n",
    "        self.A = np.block([\n",
    "            [I2,            tau_I2],\n",
    "            [np.zeros((2,2)),    I2       ]\n",
    "        ])\n",
    "\n",
    "        # Process noise Q_k,n (Eq. (11)): sigma^2 * [...]\n",
    "        q = self.sigma_process\n",
    "        self.Q = q * np.array([\n",
    "            [self.tau**3/3, 0, self.tau**2/2,             0            ],\n",
    "            [0, self.tau**3/3,      0,             self.tau**2/2       ],\n",
    "            [self.tau**2/2,             0,             self.tau, 0],\n",
    "            [0,             self.tau**2/2,             0, self.tau     ]\n",
    "        ])\n",
    "\n",
    "        # Measurement covariance = R * I2\n",
    "        self.CovMeas = self.R * np.eye(2)\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # ALGORITHM 2, STEP 0: Initialize Particle Set\n",
    "    # ----------------------------------------------------------------\n",
    "    def init_particles(self, Nmax=5):\n",
    "        r\"\"\"\n",
    "        This method creates the initial particle set, approximating\n",
    "        the distribution \\(p(x_0, e_0)\\).\n",
    "\n",
    "        The article (Algorithm 1, Step 0) does not fully specify how\n",
    "        to initialize. It says \"For the joint draw of \\{x_k, e_k, x_{k-1}, e_{k-1}\\},\n",
    "        the following proposal distribution is used...\" [Eq. (14)],\n",
    "        but the initial time step is often scenario-dependent.\n",
    "\n",
    "        \\[\n",
    "          \\text{One possible approach: all targets inactive at } k=0\n",
    "          \\quad\\Longrightarrow\\quad\n",
    "          e_0 = (0,0,\\dots,0),\\;\n",
    "          x_0 = \\text{(some default or random states)}.\n",
    "        \\]\n",
    "\n",
    "        Here, for clarity, we start each target as inactive (\\(e_0,n=0\\)),\n",
    "        and set the kinematics to zero or a user-specified default.\n",
    "        Adjust this to your prior knowledge if needed.\n",
    "        \"\"\"\n",
    "        particles = []\n",
    "        for _ in range(self.Np):\n",
    "            # By default: all targets are inactive\n",
    "            e_init = np.zeros(Nmax, dtype=int)\n",
    "            # By default: x_init is all zeros\n",
    "            x_init = np.zeros((Nmax, 4), dtype=float)\n",
    "            # log-weight initially set to 0\n",
    "            w_init = 0.0\n",
    "            particles.append({'x': x_init, 'e': e_init, 'w': w_init})\n",
    "\n",
    "        return particles\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # ALGORITHM 2, STEP 1: Sample Existence Variables (Eq. (6))\n",
    "    # ----------------------------------------------------------------\n",
    "    def sample_existence(self, e_prev):\n",
    "        r\"\"\"\n",
    "        Samples each target's existence variable \\( e_{k,n} \\) given\n",
    "        \\( e_{k-1,n} \\). This follows Eq. (6) in the paper:\n",
    "\n",
    "        \\[\n",
    "          p(e_{k,n}=1 \\mid e_{k-1,n}=1) = 1 - P_D, \\quad\n",
    "          p(e_{k,n}=0 \\mid e_{k-1,n}=1) = P_D,\n",
    "        \\]\n",
    "        \\[\n",
    "          p(e_{k,n}=1 \\mid e_{k-1,n}=0) = P_B, \\quad\n",
    "          p(e_{k,n}=0 \\mid e_{k-1,n}=0) = 1 - P_B.\n",
    "        \\]\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        e_prev : ndarray of shape (Nmax,)\n",
    "            Existence vector at the previous time step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        e_new : ndarray of shape (Nmax,)\n",
    "            Sampled existence vector at the current time step.\n",
    "        \"\"\"\n",
    "        Nmax = len(e_prev)\n",
    "        e_new = np.zeros(Nmax, dtype=int)\n",
    "\n",
    "        for n in range(Nmax):\n",
    "            if e_prev[n] == 1:\n",
    "                # e_{k-1,n} = 1 => remain alive with prob (1 - PD), or die with prob PD\n",
    "                if np.random.rand() < self.PD:\n",
    "                    e_new[n] = 0\n",
    "                else:\n",
    "                    e_new[n] = 1\n",
    "            else:\n",
    "                # e_{k-1,n} = 0 => become alive with prob PB, or stay dead with prob (1 - PB)\n",
    "                if np.random.rand() < self.PB:\n",
    "                    e_new[n] = 1\n",
    "                else:\n",
    "                    e_new[n] = 0\n",
    "\n",
    "        return e_new\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # ALGORITHM 2, STEP 2: Sample States p(x_{k,n} | x_{k-1,n}, e_{k,n}, e_{k-1,n}), Eq. (7)\n",
    "    # ----------------------------------------------------------------\n",
    "    def sample_motion(self, x_prev, e_prev, e_curr):\n",
    "        r\"\"\"\n",
    "        Samples each target's kinematic state \\(x_{k,n}\\) given\n",
    "        \\( x_{k-1,n} \\) and the existence variables \\( e_{k,n}, e_{k-1,n}\\).\n",
    "\n",
    "        From Eq. (7):\n",
    "        \\[\n",
    "          p(x_{k,n} \\mid x_{k-1,n}, e_{k,n}, e_{k-1,n}) = \n",
    "            \\begin{cases}\n",
    "              p_b(x_{k,n}) & \\text{if } \\{e_{k,n}=1, e_{k-1,n}=0\\}, \\\\\n",
    "              p_d(x_{k,n}) & \\text{if } e_{k,n}=0, \\\\\n",
    "              p_u(x_{k,n} \\mid x_{k-1,n}) & \\text{if } \\{e_{k,n}=1, e_{k-1,n}=1\\}.\n",
    "            \\end{cases}\n",
    "        \\]\n",
    "\n",
    "        where\n",
    "         - \\(p_b\\) is the birth density (Eq. (8)),\n",
    "         - \\(p_d\\) is the death density (Eq. (9)),\n",
    "         - \\(p_u\\) is the near-constant velocity update (Eq. (10)).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_prev : ndarray of shape (Nmax, 4)\n",
    "            States of the previous time step.\n",
    "        e_prev : ndarray of shape (Nmax,)\n",
    "            Existence vector at the previous time step.\n",
    "        e_curr : ndarray of shape (Nmax,)\n",
    "            Existence vector at the current time step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_curr : ndarray of shape (Nmax, 4)\n",
    "            Sampled states at the current time step.\n",
    "        \"\"\"\n",
    "        Nmax = len(e_prev)\n",
    "        x_curr = np.zeros((Nmax, 4), dtype=float)\n",
    "\n",
    "        for n in range(Nmax):\n",
    "            if (e_prev[n] == 0) and (e_curr[n] == 1):\n",
    "                # Birth => p_b(x_{k,n}), Eq. (8)\n",
    "                # Uniform in [0, Lx], [0, Ly], velocity in [-Vmax, Vmax].\n",
    "                x_birth = np.zeros(4, dtype=float)\n",
    "                x_birth[0] = np.random.uniform(0, self.Lx)               # x\n",
    "                x_birth[2] = np.random.uniform(0, self.Ly)               # y\n",
    "                x_birth[1] = np.random.uniform(-self.Vmax, self.Vmax)    # vx\n",
    "                x_birth[3] = np.random.uniform(-self.Vmax, self.Vmax)    # vy\n",
    "                x_curr[n, :] = x_birth\n",
    "\n",
    "            elif e_curr[n] == 0:\n",
    "                # Death => p_d(x_{k,n}) = δ(x_death), Eq. (9)\n",
    "                x_curr[n, :] = self.x_death\n",
    "\n",
    "            else:\n",
    "                # Update => p_u(x_{k,n} | x_{k-1,n}), Eq. (10)\n",
    "                # near-constant velocity: N(A x_{k-1,n}, Q).\n",
    "                mean = self.A @ x_prev[n, :]  # A is 4x4, x_prev[n,:] is 4x1\n",
    "                x_curr[n, :] = np.random.multivariate_normal(mean, self.Q)\n",
    "\n",
    "        return x_curr\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # LOG-LIKELIHOOD p(z_k | x_k), Eqs. (12)–(13)\n",
    "    # ----------------------------------------------------------------\n",
    "    def log_likelihood_measurements(self, z, x, e, LambdaC, LambdaX):\n",
    "        r\"\"\"\n",
    "        Computes \\(\\log p(z_k | x_k)\\) under the association-free model (Eqs. (12)–(13)):\n",
    "\n",
    "        \\[\n",
    "          p(z_k \\mid x_k) \\;=\\;\n",
    "           \\frac{\\exp(-\\mu_k)}{M_k!}\n",
    "           \\prod_{m=1}^{M_k} \\lambda(z_k^{(m)})  \\;,\\quad\n",
    "          \\text{where } \\mu_k = \\Lambda_C + \\sum_{n \\in \\text{active}} \\Lambda_X,\n",
    "        \\]\n",
    "        \\[\n",
    "          \\lambda(z) = \\Lambda_C\\, p_C(z) \\;+\\; \n",
    "                        \\sum_{n \\in \\text{active}} \\bigl[\\Lambda_X\\, p_x(z \\mid x_{k,n})\\bigr],\n",
    "        \\]\n",
    "        with\n",
    "         - \\(p_C(z)\\) uniform in \\([0,L_x]\\times [0,L_y]\\),\n",
    "         - \\(p_x(z \\mid x_{k,n}) = \\mathcal{N}(z; [x_{k,n}, y_{k,n}], \\text{CovMeas})\\).\n",
    "\n",
    "        We do everything in the log domain for numerical stability.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z : ndarray, shape (M_k, 2)\n",
    "            All measurements at time k.\n",
    "        x : ndarray, shape (Nmax, 4)\n",
    "            The states (x, vx, y, vy) for each target.\n",
    "        e : ndarray, shape (Nmax,)\n",
    "            Existence indicator for each target (0 or 1).\n",
    "        LambdaC : float\n",
    "            Mean clutter rate \\(\\Lambda_C\\).\n",
    "        LambdaX : float\n",
    "            Mean detection rate per active target \\(\\Lambda_X\\).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        log_like : float\n",
    "            The log of p(z_k | x_k).\n",
    "        \"\"\"\n",
    "        # Number of active targets\n",
    "        N_active = np.sum(e)\n",
    "        # mu_k = LambdaC + N_active * LambdaX\n",
    "        mu_k = LambdaC + N_active * LambdaX\n",
    "        # M_k = number of measurements\n",
    "        M_k = z.shape[0]\n",
    "\n",
    "        # log(M_k!)\n",
    "        if M_k <= 1:\n",
    "            log_factorial_M = 0.0\n",
    "        else:\n",
    "            log_factorial_M = np.sum(np.log(np.arange(1, M_k+1)))\n",
    "\n",
    "        # Start log p(z_k|x_k)\n",
    "        #  = - mu_k - log(M_k!) + sum_{m=1..M_k} [ log(lambda(z_m)) ]\n",
    "        log_like = -mu_k - log_factorial_M\n",
    "\n",
    "        # Uniform clutter density = 1 / (Lx * Ly)\n",
    "        clutter_density = 1.0 / (self.Lx * self.Ly)\n",
    "\n",
    "        # Utility for computing Gaussian pdf in 2D\n",
    "        def normal_pdf_2d(zm, mean_xy, Cov):\n",
    "            return multivariate_normal.pdf(zm, mean=mean_xy, cov=Cov)\n",
    "\n",
    "        for m in range(M_k):\n",
    "            zm = z[m, :]  # measurement\n",
    "            # lambda(z_m) = LambdaC * pC(z_m) + sum_{active n} [ LambdaX * px(z_m| x_{k,n}) ]\n",
    "            lam_val = LambdaC * clutter_density\n",
    "\n",
    "            for n in range(len(e)):\n",
    "                if e[n] == 1:\n",
    "                    # x[n,:] = (x, vx, y, vy)\n",
    "                    mean_xy = np.array([x[n, 0], x[n, 2]], dtype=float)\n",
    "                    lam_val += LambdaX * normal_pdf_2d(zm, mean_xy, self.CovMeas)\n",
    "\n",
    "            # If lam_val ~ 0 => log -> -inf\n",
    "            if lam_val < 1e-300:\n",
    "                return -1e16\n",
    "            log_like += np.log(lam_val)\n",
    "\n",
    "        return log_like\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Main Filtering Loop (MCMC-based Particle Algorithm)\n",
    "    # ---------------------------------------------------------\n",
    "    def filter(self, measurements, Nmax=5):\n",
    "        \"\"\"\n",
    "        Perform the MCMC-based particle filtering over a sequence of measurements\n",
    "        according to the approach in Section III-B and Algorithm 1 of the paper.\n",
    "\n",
    "        Steps:\n",
    "          1) For k=0..K-1:\n",
    "             (a) Prediction: sample e_k from e_{k-1} (Eq. (6)) & x_k from x_{k-1} (Eq. (7))\n",
    "             (b) Joint draw (Eq. (14)) => predicted particles\n",
    "             (c) Refinement: MCMC step that successively samples each target's {x_{k,n}, e_{k,n}}\n",
    "                 using q3(...) = p(x_{k,n} | x_{k-1,n}, e_{k,n}) p(e_{k,n} | e_{k-1,n}) (Eq. (15))\n",
    "             (d) Weight update with p(z_k | x_k) from Eqs. (12)–(13), in log form\n",
    "             (e) Resampling\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        all_particles : list of length K\n",
    "            all_particles[k] is the final particle set for time k.\n",
    "        card_mean : array of shape (K,)\n",
    "            Mean cardinality (number of alive targets) per time step.\n",
    "        card_std : array of shape (K,)\n",
    "            Standard deviation of the cardinality estimates.\n",
    "        \"\"\"\n",
    "        K = len(measurements)\n",
    "        # Initialize from (possibly) q1(...) (Eq. (14))\n",
    "        # Here we simply start all in a default \"dead\" state.\n",
    "        particles_prev = self.init_particles(Nmax=Nmax)\n",
    "\n",
    "        card_mean = np.zeros(K)\n",
    "        card_std = np.zeros(K)\n",
    "        all_particles = []\n",
    "\n",
    "        # Optional progress bar (from tqdm) for user convenience\n",
    "        for k in tqdm(range(K), desc=\"MCMC-PF Filtering\"):\n",
    "            z_k = measurements[k]\n",
    "\n",
    "            # ---- (a) & (b) Prediction Step & Joint Draw (Eq. (14)) ----\n",
    "            predicted = []\n",
    "            for i in range(self.Np):\n",
    "                x_old = particles_prev[i]['x']\n",
    "                e_old = particles_prev[i]['e']\n",
    "                w_old = particles_prev[i]['w']  # log-weight from previous\n",
    "\n",
    "                # Sample e_k from e_{k-1} (Eq. (6))\n",
    "                e_new = self.sample_existence(e_old)\n",
    "                # Sample x_k from x_{k-1}, e_k, e_{k-1} (Eq. (7))\n",
    "                x_new = self.sample_motion(x_old, e_old, e_new)\n",
    "\n",
    "                predicted.append({'x': x_new, 'e': e_new, 'w': w_old})\n",
    "\n",
    "            # ---- (c) Refinement Step (Eq. (15)) ----\n",
    "            # For each particle, run a short MCMC chain that updates each target's {x_{k,n}, e_{k,n}} successively.\n",
    "            # We skip re-drawing x_{k-1}, e_{k-1} (q2 not used in this simplified version).\n",
    "            for i in range(self.Np):\n",
    "                x_cur = predicted[i]['x'].copy()\n",
    "                e_cur = predicted[i]['e'].copy()\n",
    "\n",
    "                # Basic MCMC with Nburn + thinning * 1 (or more) steps\n",
    "                # Each iteration, we cycle through targets n=1..Nmax\n",
    "                for _ in range(self.Nburn + self.thinning):\n",
    "                    for n in range(Nmax):\n",
    "                        e_oldn = e_cur[n]\n",
    "                        x_oldn = x_cur[n, :].copy()\n",
    "\n",
    "                        # Propose a local change in existence or state:\n",
    "                        # The article uses q3(...) = p(x_k,n | x_{k-1,n}, e_k,n) p(e_k,n| e_{k-1,n}).\n",
    "                        # Here, we do a small random move if alive, or keep x_death if dead.\n",
    "                        if e_oldn == 1:\n",
    "                            # local small perturbation\n",
    "                            x_prop_n = x_oldn + 0.05 * randn(4)\n",
    "                        else:\n",
    "                            x_prop_n = self.x_death.copy()\n",
    "\n",
    "                        # Evaluate log-likelihood before\n",
    "                        ll_old = self.log_likelihood_measurements(z_k, x_cur, e_cur, self.LambdaC, self.LambdaX)\n",
    "\n",
    "                        # Modify just the nth target's state\n",
    "                        x_test = x_cur.copy()\n",
    "                        x_test[n, :] = x_prop_n\n",
    "                        # We could also consider flipping e_cur[n] with small probability\n",
    "                        # but typically that would require referencing e_{k-1,n}; omitted for brevity.\n",
    "\n",
    "                        ll_new = self.log_likelihood_measurements(z_k, x_test, e_cur, self.LambdaC, self.LambdaX)\n",
    "\n",
    "                        # Accept/reject (Metropolis-Hastings)\n",
    "                        alpha = np.exp(ll_new - ll_old)\n",
    "                        if np.random.rand() < alpha:\n",
    "                            x_cur[n, :] = x_prop_n\n",
    "\n",
    "                # After MCMC chain, compute final log-likelihood\n",
    "                w_final = self.log_likelihood_measurements(z_k, x_cur, e_cur, self.LambdaC, self.LambdaX)\n",
    "                predicted[i]['x'] = x_cur\n",
    "                predicted[i]['e'] = e_cur\n",
    "                predicted[i]['w'] = w_final\n",
    "\n",
    "            # ---- (d) Resampling ----\n",
    "            weights_log = np.array([p['w'] for p in predicted])\n",
    "            wmax = np.max(weights_log)\n",
    "            # Convert log-weights to normalized weights\n",
    "            weights_lin = np.exp(weights_log - wmax)\n",
    "            weights_lin /= np.sum(weights_lin)\n",
    "\n",
    "            indices = choice(self.Np, size=self.Np, replace=True, p=weights_lin)\n",
    "            new_particles = []\n",
    "            for idx in indices:\n",
    "                # Copy the chosen particle; set new log-weight=0 after resampling\n",
    "                new_particles.append({\n",
    "                    'x': predicted[idx]['x'].copy(),\n",
    "                    'e': predicted[idx]['e'].copy(),\n",
    "                    'w': 0.0\n",
    "                })\n",
    "\n",
    "            # ---- (e) Cardinality Estimation ----\n",
    "            card_est = [np.sum(p['e']) for p in new_particles]\n",
    "            card_mean[k] = np.mean(card_est)\n",
    "            card_std[k] = np.std(card_est)\n",
    "\n",
    "            # Move to next time step\n",
    "            particles_prev = new_particles\n",
    "            all_particles.append(new_particles)\n",
    "\n",
    "        return all_particles, card_mean, card_std\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4) HELPER FUNCTION: TAKE THE MEAN OF PARTICLES\n",
    "# ----------------------------------------------------------------------\n",
    "def extract_average_particle_tracks(particle_set):\n",
    "    \"\"\"\n",
    "    Compute the weighted average of the particle states (x) and\n",
    "    the (fractional) existence (e) at a single time step.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    particle_set : list of dict\n",
    "        Each dict has keys:\n",
    "          'x': (Nmax, 4) - the kinematic state,\n",
    "          'e': (Nmax,) in {0,1} - existence indicators,\n",
    "          'w': float (the log-weight).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_avg : numpy array, shape=(Nmax, 4)\n",
    "        The weighted average of the kinematic states.\n",
    "    e_avg : numpy array, shape=(Nmax,)\n",
    "        The (fractional) weighted average of the existences, i.e. in [0,1].\n",
    "        (If you want hard 0/1, you can threshold this later.)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Gather log-weights in a NumPy array\n",
    "    log_w = np.array([p['w'] for p in particle_set], dtype=float)\n",
    "\n",
    "    # 2) Convert to linear weights safely (avoid exp underflow)\n",
    "    w_max = np.max(log_w)\n",
    "    w_lin = np.exp(log_w - w_max)\n",
    "\n",
    "    # 3) Normalize the weights\n",
    "    w_lin_sum = np.sum(w_lin)\n",
    "    if w_lin_sum < 1e-300:\n",
    "        # If all weights are extremely small or -inf, fallback:\n",
    "        w_lin = np.ones_like(w_lin) / len(w_lin_sum)\n",
    "        w_lin_sum = 1.0\n",
    "    else:\n",
    "        w_lin /= w_lin_sum\n",
    "\n",
    "    # 4) Compute weighted average for x and e\n",
    "    #    We'll accumulate sums in arrays\n",
    "    #    x_sum will be shape (Nmax, 4), e_sum will be shape (Nmax,)\n",
    "    Nmax = particle_set[0]['x'].shape[0]\n",
    "    x_sum = np.zeros((Nmax, 4), dtype=float)\n",
    "    e_sum = np.zeros(Nmax, dtype=float)\n",
    "\n",
    "    for i, part in enumerate(particle_set):\n",
    "        x_sum += w_lin[i] * part['x']\n",
    "        e_sum += w_lin[i] * part['e']\n",
    "\n",
    "    x_avg = x_sum  # Weighted mean\n",
    "    e_avg = e_sum  # Weighted mean in [0,1]\n",
    "\n",
    "    return x_avg, e_avg\n",
    "\n",
    "\n",
    "def get_estimated_tracks_over_time(all_particles):\n",
    "    \"\"\"\n",
    "    Use the weighted average of the particles at each time k to produce\n",
    "    the estimated track over time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    all_particles : list of lists\n",
    "        all_particles[k] is the particle set at time k, \n",
    "        i.e. a list of dicts [{'x':..., 'e':..., 'w':...}, ...].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_est : numpy array, shape=(K, Nmax, 4)\n",
    "        Weighted-average states at each time k.\n",
    "    e_est : numpy array, shape=(K, Nmax)\n",
    "        Weighted-average existences (in [0,1]) at each time k.\n",
    "        (You can threshold later if you need strict 0/1.)\n",
    "    \"\"\"\n",
    "    K = len(all_particles)\n",
    "    Nmax = all_particles[0][0]['x'].shape[0]\n",
    "\n",
    "    x_est = np.zeros((K, Nmax, 4), dtype=float)\n",
    "    e_est = np.zeros((K, Nmax), dtype=float)\n",
    "\n",
    "    for k in range(K):\n",
    "        x_k, e_k = extract_average_particle_tracks(all_particles[k])\n",
    "        x_est[k, :, :] = x_k\n",
    "        e_est[k, :] = e_k\n",
    "\n",
    "    return x_est, e_est\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 5) MAIN SIMULATION AND PLOTTING\n",
    "# ----------------------------------------------------------------------\n",
    "def main_simulation(K=80, do_plot=True):\n",
    "    \"\"\"\n",
    "    Executes the complete simulation in strict accordance with the article's methodology:\n",
    "     1) Generate ground truth for a maximum of 5 targets (births/deaths, near-constant velocity).\n",
    "     2) Generate measurements (association-free, Poisson-based).\n",
    "     3) Instantiate the MCMC-based Particle Filter with chosen parameters (Np, Nburn, etc.).\n",
    "     4) Perform filtering over the measurement sequence (Algorithm 1).\n",
    "     5) Extract a single estimated track using the highest-weight particle at each time step.\n",
    "     6) Optionally plot:\n",
    "        - True vs. estimated cardinality\n",
    "        - True vs. estimated x-y trajectories\n",
    "    \"\"\"\n",
    "    # 1) Generate ground truth (Eq. (10), Section IV)\n",
    "    ground_truth = generate_ground_truth(K=K)\n",
    "\n",
    "    # 2) Generate measurements (Eqs. (12)–(13), association-free model)\n",
    "    measurements = generate_measurements(ground_truth)\n",
    "\n",
    "    # 3) Instantiate MCMC-based Particle Filter with smaller defaults\n",
    "    #    (the article used Np=4000, Nburn=1000, etc. for final results,\n",
    "    #     but here we reduce them for a quicker demo).\n",
    "    pf = MCMCParticleFilter(Np=100,    # fewer particles for demo\n",
    "                            Nburn=2,  # fewer burn-in steps\n",
    "                            thinning=1,\n",
    "                            PB=0.1,\n",
    "                            PD=0.1)\n",
    "\n",
    "    # 4) Run the particle filter\n",
    "    all_particles, card_mean, card_std = pf.filter(measurements, Nmax=5)\n",
    "\n",
    "    # 5) Extract track estimates (best-weight particle) over time\n",
    "    x_est, e_est = get_estimated_tracks_over_time(all_particles)\n",
    "\n",
    "    # 6) (Optional) Plot results\n",
    "    if do_plot:\n",
    "        import matplotlib.pyplot as plt\n",
    "        e_true = ground_truth['e_true']  # shape (K, Nmax)\n",
    "        x_true = ground_truth['x_true']  # shape (K, Nmax, 4)\n",
    "        true_card = np.sum(e_true, axis=1)\n",
    "\n",
    "        # (A) Plot the cardinality\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(true_card, 'k-', label='True Cardinality')\n",
    "        plt.plot(card_mean, 'r--', label='Estimated Mean Cardinality')\n",
    "        plt.fill_between(\n",
    "            range(K),\n",
    "            card_mean - card_std,\n",
    "            card_mean + card_std,\n",
    "            color='r',\n",
    "            alpha=0.2,\n",
    "            label='±1 std'\n",
    "        )\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Cardinality')\n",
    "        plt.title('Cardinality: True vs. Estimated')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # (B) Plot the true and estimated tracks in x-y plane\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        # Plot each true target\n",
    "        for n in range(e_true.shape[1]):\n",
    "            alive_times = np.where(e_true[:, n] == 1)[0]\n",
    "            if len(alive_times) > 0:\n",
    "                # Récupérer le vecteur x et y de la cible n\n",
    "                xvals = x_true[alive_times, n, 0]\n",
    "                yvals = x_true[alive_times, n, 2]\n",
    "                \n",
    "                # Indices de début et fin\n",
    "                k_start = alive_times[0]\n",
    "                k_stop = alive_times[-1]\n",
    "                \n",
    "                plt.plot(xvals, yvals, '-o', label=f\"True T{n+1}\")\n",
    "                \n",
    "                # Ajouter un marqueur cercle (o) pour la naissance\n",
    "                plt.plot(x_true[k_start, n, 0],\n",
    "                        x_true[k_start, n, 2],\n",
    "                        'ko',  # k = noir, o = cercle\n",
    "                        markersize=8)\n",
    "\n",
    "                # Ajouter un marqueur triangle (Δ) pour la mort\n",
    "                # (seulement si la cible meurt effectivement)\n",
    "                if k_stop < (e_true.shape[0] - 1):  \n",
    "                    plt.plot(x_true[k_stop, n, 0],\n",
    "                            x_true[k_stop, n, 2],\n",
    "                            'k^',  # k = noir, ^ = triangle\n",
    "                            markersize=8)\n",
    "\n",
    "        # Plot each estimated target from best-weight particle\n",
    "        for n in range(e_est.shape[1]):\n",
    "            alive_times_est = np.where(e_est[:, n] == 1)[0]\n",
    "            if len(alive_times_est) > 0:\n",
    "                plt.plot(\n",
    "                    x_est[alive_times_est, n, 0],\n",
    "                    x_est[alive_times_est, n, 2],\n",
    "                    '--x',\n",
    "                    label=f\"Est T{n+1}\"\n",
    "                )\n",
    "\n",
    "        plt.xlabel('x [m]')\n",
    "        plt.ylabel('y [m]')\n",
    "        plt.xlim([0, 5000])\n",
    "        plt.ylim([0, 5000])\n",
    "        plt.title('Tracks in x-y Plane')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return ground_truth, measurements, (card_mean, card_std), x_est, e_est\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 6) RUN EXAMPLE\n",
    "# ----------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: single run, T=100\n",
    "    T = 100\n",
    "    gt, meas, (c_mean, c_std), x_estimated, e_estimated = main_simulation(K=T, do_plot=True)\n",
    "\n",
    "    # The user can adapt the code to do multiple runs or compute advanced metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MCMC-Based Particle Filter for Multi-Target Tracking with Enhanced Plotting\n",
    "===========================================================================\n",
    "This script demonstrates:\n",
    " 1) Ground-truth generation for up to 5 targets (with births/deaths).\n",
    " 2) Poisson clutter + target-origin measurements.\n",
    " 3) An MCMC-based particle filter to estimate the (x, y) positions and (vx, vy) velocities\n",
    "    of each target, and whether each target exists or not at each time step.\n",
    " 4) Enhanced Plots:\n",
    "    - True tracks vs. estimated tracks in x-y plane.\n",
    "    - Cardinality (true vs. estimated) over time.\n",
    "    - Existence probabilities over time.\n",
    "    - Root Mean Square Error (RMSE) computation.\n",
    "    - Measurements and estimates at selected time steps.\n",
    " 5) A progress bar (via tqdm) to indicate the filtering progress.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import rand, randn, choice\n",
    "from scipy.stats import poisson, multivariate_normal\n",
    "from tqdm import tqdm  # for progress bar\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1) GROUND-TRUTH GENERATION\n",
    "# ----------------------------------------------------------------------\n",
    "def generate_ground_truth(K=80,  # total number of timesteps\n",
    "                          tau=3.0,\n",
    "                          sigma_process=0.5,\n",
    "                          scenario_params=None):\n",
    "    \"\"\"\n",
    "    Generate ground-truth states for up to 5 targets, with known birth times\n",
    "    and at least one known death time. Targets follow a near-constant velocity (NCV) model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    K : int\n",
    "        Number of timesteps.\n",
    "    tau : float\n",
    "        Sampling interval (seconds).\n",
    "    sigma_process : float\n",
    "        Standard deviation of the process noise for NCV model.\n",
    "    scenario_params : dict\n",
    "        Dictionary specifying each target's birth_time, death_time, and initial state.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ground_truth : dict\n",
    "        Contains:\n",
    "          x_true : array, shape=(K, Nmax, 4), the true states for each target\n",
    "                   (x, vx, y, vy).\n",
    "          e_true : array, shape=(K, Nmax), the existence indicators (0 or 1).\n",
    "    \"\"\"\n",
    "\n",
    "    # Default scenario: 5 targets\n",
    "    #   T1, T2, T3 born at k=1\n",
    "    #   T4, T5 born at k=25\n",
    "    #   T1 dies at k=50\n",
    "    if scenario_params is None:\n",
    "        scenario_params = {\n",
    "            0: {'birth_time': 1,\n",
    "                'death_time': 50,\n",
    "                'init_state': np.array([500.0,   1.0,  500.0,   15.0])},  \n",
    "            1: {'birth_time': 1,\n",
    "                'death_time': None,\n",
    "                'init_state': np.array([1000.0, -1.0, 4000.0, -10.0])},\n",
    "            2: {'birth_time': 1,\n",
    "                'death_time': None,\n",
    "                'init_state': np.array([4000.0,   5.0, 1000.0,   5.0])},\n",
    "            3: {'birth_time': 25,\n",
    "                'death_time': None,\n",
    "                'init_state': np.array([4500.0,  -5.0, 4500.0,   5.0])},\n",
    "            4: {'birth_time': 25,\n",
    "                'death_time': None,\n",
    "                'init_state': np.array([500.0,   10.0, 4500.0,  -15.0])}\n",
    "        }\n",
    "\n",
    "    Nmax = len(scenario_params)  # should be 5\n",
    "    x_true = np.zeros((K, Nmax, 4))\n",
    "    e_true = np.zeros((K, Nmax), dtype=int)\n",
    "\n",
    "    # Define the state transition matrix for near-constant velocity (NCV)\n",
    "    I_2 = np.array([[1, 0],\n",
    "                    [0, 1]], dtype=float)\n",
    "    tau_I_2 = np.array([[tau, 0],\n",
    "                        [0, tau]], dtype=float)\n",
    "    A = np.block([[I_2,           tau_I_2],\n",
    "                  [np.zeros((2, 2)), I_2          ]])\n",
    "    # Process noise covariance\n",
    "    q = sigma_process**2\n",
    "    Q = q * np.array([[tau**3/3, 0, tau**2/2,         0],\n",
    "                      [0,       tau**3/3, 0,         tau**2/2],\n",
    "                      [tau**2/2,        0,         tau, 0],\n",
    "                      [0,        tau**2/2,         0,       tau]])\n",
    "\n",
    "    # Initialize states for the first timestep\n",
    "    for n in range(Nmax):\n",
    "        bt = scenario_params[n]['birth_time']\n",
    "        dt = scenario_params[n]['death_time']\n",
    "        x0 = scenario_params[n]['init_state'].copy()\n",
    "\n",
    "        if bt is not None and 1 >= bt:\n",
    "            x_true[0, n, :] = x0\n",
    "            e_true[0, n] = 1\n",
    "\n",
    "    # Generate ground truth for subsequent timesteps\n",
    "    for k in range(1, K):\n",
    "        for n in range(Nmax):\n",
    "            bt = scenario_params[n]['birth_time']\n",
    "            dt = scenario_params[n]['death_time']\n",
    "\n",
    "            if bt is not None and k >= bt and (dt is None or k < dt):\n",
    "                # If k == bt, then the target is just born at this step\n",
    "                if k == bt:\n",
    "                    x_true[k, n, :] = scenario_params[n]['init_state']\n",
    "                    e_true[k, n] = 1\n",
    "                else:\n",
    "                    # Continue the NCV model\n",
    "                    x_prev = x_true[k-1, n, :]\n",
    "                    mean = A @ x_prev\n",
    "                    x_true[k, n, :] = np.random.multivariate_normal(mean, Q)\n",
    "                    e_true[k, n] = 1\n",
    "            else:\n",
    "                # Target does not exist at time k\n",
    "                x_true[k, n, :] = 0.0\n",
    "                e_true[k, n] = 0\n",
    "\n",
    "    return {'x_true': x_true, 'e_true': e_true}\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2) MEASUREMENT GENERATION\n",
    "# ----------------------------------------------------------------------\n",
    "def generate_measurements(ground_truth,\n",
    "                          Lx=5000.0, \n",
    "                          Ly=5000.0,\n",
    "                          LambdaC=20.0,  # Mean clutter rate (Poisson)\n",
    "                          LambdaX=1.0,   # Mean detection rate per active target (Poisson)\n",
    "                          Sigma_x=100.0, # Measurement variance (Sigma_x * I2)\n",
    "                          seed=None):\n",
    "    \"\"\"\n",
    "    Generate a set of measurements for each time step `k` based on the\n",
    "    'association-free' model described in the article.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ground_truth : dict\n",
    "        Dictionary containing at least:\n",
    "          - ground_truth['x_true']: array of shape (K, Nmax, 4),\n",
    "            where x_true[k, n, :] = (x_k,n, vx_k,n, y_k,n, vy_k,n).\n",
    "          - ground_truth['e_true']: array of shape (K, Nmax),\n",
    "            where e_true[k, n] = 1 if target `n` exists at time `k`, otherwise 0.\n",
    "    Lx, Ly : float\n",
    "        Dimensions of the surveillance region in meters.\n",
    "    LambdaC : float\n",
    "        Mean number of clutter (false alarms) per time step.\n",
    "    LambdaX : float\n",
    "        Mean number of detections per active target and per time step.\n",
    "    Sigma_x : float\n",
    "        Measurement variance in both x and y directions (covariance is Sigma_x * I2).\n",
    "    seed : int or None\n",
    "        Seed for the random number generator (for reproducibility).\n",
    "        If None, results will be random.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    measurements : list\n",
    "        A list of length K.\n",
    "        measurements[k] is a numpy array of shape (M_k, 2),\n",
    "        where M_k is the total number of measurements at time step `k` (target + clutter).\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)  # For optional reproducibility\n",
    "\n",
    "    x_true = ground_truth['x_true']  # (K, Nmax, 4)\n",
    "    e_true = ground_truth['e_true']  # (K, Nmax)\n",
    "    K, Nmax, _ = x_true.shape\n",
    "\n",
    "    # Measurement covariance matrix: Sigma_x * I2\n",
    "    R_meas = Sigma_x * np.eye(2)\n",
    "\n",
    "    # List to store measurements for each time step k\n",
    "    measurements = []\n",
    "\n",
    "    # Loop through each time step k\n",
    "    for k in range(K):\n",
    "        # -- 1) Identify active targets\n",
    "        active_targets = np.where(e_true[k, :] == 1)[0]\n",
    "\n",
    "        # -- 2) Generate target-originated measurements\n",
    "        #     For each active target, draw the number of measurements from Poisson(LambdaX).\n",
    "        target_meas_list = []\n",
    "        for n in active_targets:\n",
    "            # Number of measurements for target `n`\n",
    "            count_target_n = poisson.rvs(LambdaX)\n",
    "            if count_target_n > 0:\n",
    "                # Mean of the Gaussian distribution for target `n`: (x_k,n, y_k,n)\n",
    "                mean_xy = np.array([\n",
    "                    x_true[k, n, 0],  # x_k,n\n",
    "                    x_true[k, n, 2]   # y_k,n\n",
    "                ])\n",
    "                # Generate Gaussian measurements\n",
    "                z_n = np.random.multivariate_normal(mean_xy, R_meas, size=count_target_n)\n",
    "                target_meas_list.append(z_n)\n",
    "\n",
    "        # -- 3) Generate clutter (false alarms)\n",
    "        #     Draw the number of clutter points from Poisson(LambdaC).\n",
    "        clutter_count = poisson.rvs(LambdaC)\n",
    "        # Clutter measurements are uniformly distributed in [0, Lx] x [0, Ly].\n",
    "        clutter_meas = np.column_stack([\n",
    "            np.random.uniform(0, Lx, size=clutter_count),\n",
    "            np.random.uniform(0, Ly, size=clutter_count)\n",
    "        ])\n",
    "\n",
    "        # -- 4) Combine target and clutter measurements\n",
    "        if len(target_meas_list) > 0:\n",
    "            all_target_meas = np.vstack(target_meas_list)  # Concatenate all target measurements\n",
    "            all_meas = np.vstack([all_target_meas, clutter_meas])\n",
    "        else:\n",
    "            # If there are no active targets or Poisson(LambdaX)=0, only clutter measurements remain\n",
    "            all_meas = clutter_meas\n",
    "\n",
    "        measurements.append(all_meas)\n",
    "\n",
    "    return measurements\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3) MCMC-BASED PARTICLE FILTER CLASS\n",
    "# ----------------------------------------------------------------------\n",
    "class MCMCParticleFilter:\n",
    "    \"\"\"\n",
    "    MCMC-Based Particle Filter for a time-varying number of targets, as described\n",
    "    in Section III–IV of the article.\n",
    "\n",
    "    The targets are modeled with:\n",
    "      - Existence variables e_{k,n} in {0,1} (Eq. (6))\n",
    "      - Kinematics x_{k,n} = (x, vx, y, vy) (fixed dimension Nmax x 4)\n",
    "      - Transition densities:\n",
    "         * Birth:   Eq. (8)\n",
    "         * Death:   Eq. (9)\n",
    "         * Update:  Eq. (10)\n",
    "      - Association-free measurement model: Eqs. (12)–(13)\n",
    "      - MCMC approach: Eqs. (14)–(15) for refinement\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 Np=4000,\n",
    "                 Nburn=1000,\n",
    "                 thinning=6,\n",
    "                 PB=0.01,       # Probability of birth, Eq. (6)\n",
    "                 PD=0.01,       # Probability of death, Eq. (6)\n",
    "                 sigma_process=0.5,  # std. dev. squared in NCV model\n",
    "                 tau=3.0,       # sampling interval\n",
    "                 R=100.0,       # measurement noise covariance = 100 * I2\n",
    "                 LambdaC=20.0,  # clutter rate\n",
    "                 LambdaX=1.0,   # detection rate per active target\n",
    "                 Lx=5000.0,\n",
    "                 Ly=5000.0,\n",
    "                 x_death=None,\n",
    "                 Vmax=20.0):    # max speed for birth sampling\n",
    "        \"\"\"\n",
    "        Initialize MCMC-based Particle Filter with default parameters from the article:\n",
    "          - Np=4000 (num. particles)\n",
    "          - Nburn=1000 (burn-in for MCMC)\n",
    "          - thinning=6\n",
    "          - PB, PD in [0,1]\n",
    "          - sigma_process=0.5\n",
    "          - R=100 => CovMeas = 100 * I2\n",
    "          - LambdaC=20, LambdaX=1 => Poisson parameters for clutter/target measurements\n",
    "          - Lx=5000, Ly=5000 => surveillance area\n",
    "          - x_death: placeholder state for 'dead' target (Eq. (9))\n",
    "          - Vmax=20 => uniform speed range in [-Vmax, Vmax] at birth (Eq. (8)).\n",
    "        \"\"\"\n",
    "        self.Np = Np\n",
    "        self.Nburn = Nburn\n",
    "        self.thinning = thinning\n",
    "\n",
    "        # Eq. (6) parameters for birth/death processes\n",
    "        self.PB = PB  # Probability to become alive if previously dead\n",
    "        self.PD = PD  # Probability to become dead if previously alive\n",
    "\n",
    "        # NCV model parameters (Eq. (10) and (11))\n",
    "        self.sigma_process = sigma_process\n",
    "        self.tau = tau\n",
    "\n",
    "        # Observation model parameters\n",
    "        self.R = R\n",
    "        self.LambdaC = LambdaC\n",
    "        self.LambdaX = LambdaX\n",
    "\n",
    "        # Surveillance area and speeds\n",
    "        self.Lx = Lx\n",
    "        self.Ly = Ly\n",
    "        self.Vmax = Vmax\n",
    "\n",
    "        # \"Death\" state for inactive targets (Eq. (9))\n",
    "        # By default, use zero for all (x,y,vx,vy).\n",
    "        if x_death is None:\n",
    "            self.x_death = np.zeros(4, dtype=float)\n",
    "        else:\n",
    "            self.x_death = x_death\n",
    "\n",
    "        # Near-constant velocity (NCV) state transition matrix A_k,n (Eq. (11))\n",
    "        # A = [[1, 0, tau,   0  ],\n",
    "        #      [0,  1,   0,  tau ],\n",
    "        #      [0,  0,   1,  0],\n",
    "        #      [0,  0,   0,  1 ]]\n",
    "        tau_I2 = np.array([[self.tau, 0],\n",
    "                            [0, self.tau]], dtype=float)\n",
    "        I2 = np.array([[1, 0],\n",
    "                      [0, 1]], dtype=float)\n",
    "        self.A = np.block([\n",
    "            [I2,            tau_I2],\n",
    "            [np.zeros((2,2)),    I2       ]\n",
    "        ])\n",
    "\n",
    "        # Process noise Q_k,n (Eq. (11)): sigma^2 * [...]\n",
    "        q = self.sigma_process\n",
    "        self.Q = q * np.array([\n",
    "            [self.tau**3/3, 0, self.tau**2/2,             0            ],\n",
    "            [0, self.tau**3/3,      0,             self.tau**2/2       ],\n",
    "            [self.tau**2/2,             0,             self.tau, 0],\n",
    "            [0,             self.tau**2/2,             0, self.tau     ]\n",
    "        ])\n",
    "\n",
    "        # Measurement covariance = R * I2\n",
    "        self.CovMeas = self.R * np.eye(2)\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # ALGORITHM 2, STEP 0: Initialize Particle Set\n",
    "    # ----------------------------------------------------------------\n",
    "    def init_particles(self, Nmax=5):\n",
    "        r\"\"\"\n",
    "        This method creates the initial particle set, approximating\n",
    "        the distribution \\(p(x_0, e_0)\\).\n",
    "\n",
    "        The article (Algorithm 1, Step 0) does not fully specify how\n",
    "        to initialize. It says \"For the joint draw of \\{x_k, e_k, x_{k-1}, e_{k-1}\\},\n",
    "        the following proposal distribution is used...\" [Eq. (14)],\n",
    "        but the initial time step is often scenario-dependent.\n",
    "\n",
    "        \\[\n",
    "          \\text{One possible approach: all targets inactive at } k=0\n",
    "          \\quad\\Longrightarrow\\quad\n",
    "          e_0 = (0,0,\\dots,0),\\;\n",
    "          x_0 = \\text{(some default or random states)}.\n",
    "        \\]\n",
    "\n",
    "        Here, for clarity, we start each target as inactive (\\(e_0,n=0\\)),\n",
    "        and set the kinematics to zero or a user-specified default.\n",
    "        Adjust this to your prior knowledge if needed.\n",
    "        \"\"\"\n",
    "        particles = []\n",
    "        for _ in range(self.Np):\n",
    "            # By default: all targets are inactive\n",
    "            e_init = np.zeros(Nmax, dtype=int)\n",
    "            # By default: x_init is all zeros\n",
    "            x_init = np.zeros((Nmax, 4), dtype=float)\n",
    "            # log-weight initially set to 0\n",
    "            w_init = 0.0\n",
    "            particles.append({'x': x_init, 'e': e_init, 'w': w_init})\n",
    "\n",
    "        return particles\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # ALGORITHM 2, STEP 1: Sample Existence Variables (Eq. (6))\n",
    "    # ----------------------------------------------------------------\n",
    "    def sample_existence(self, e_prev):\n",
    "        r\"\"\"\n",
    "        Samples each target's existence variable \\( e_{k,n} \\) given\n",
    "        \\( e_{k-1,n} \\). This follows Eq. (6) in the paper:\n",
    "\n",
    "        \\[\n",
    "          p(e_{k,n}=1 \\mid e_{k-1,n}=1) = 1 - P_D, \\quad\n",
    "          p(e_{k,n}=0 \\mid e_{k-1,n}=1) = P_D,\n",
    "        \\]\n",
    "        \\[\n",
    "          p(e_{k,n}=1 \\mid e_{k-1,n}=0) = P_B, \\quad\n",
    "          p(e_{k,n}=0 \\mid e_{k-1,n}=0) = 1 - P_B.\n",
    "        \\]\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        e_prev : ndarray of shape (Nmax,)\n",
    "            Existence vector at the previous time step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        e_new : ndarray of shape (Nmax,)\n",
    "            Sampled existence vector at the current time step.\n",
    "        \"\"\"\n",
    "        Nmax = len(e_prev)\n",
    "        e_new = np.zeros(Nmax, dtype=int)\n",
    "\n",
    "        for n in range(Nmax):\n",
    "            if e_prev[n] == 1:\n",
    "                # e_{k-1,n} = 1 => remain alive with prob (1 - PD), or die with prob PD\n",
    "                if np.random.rand() < self.PD:\n",
    "                    e_new[n] = 0\n",
    "                else:\n",
    "                    e_new[n] = 1\n",
    "            else:\n",
    "                # e_{k-1,n} = 0 => become alive with prob PB, or stay dead with prob (1 - PB)\n",
    "                if np.random.rand() < self.PB:\n",
    "                    e_new[n] = 1\n",
    "                else:\n",
    "                    e_new[n] = 0\n",
    "\n",
    "        return e_new\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # ALGORITHM 2, STEP 2: Sample States p(x_{k,n} | x_{k-1,n}, e_{k,n}, e_{k-1,n}), Eq. (7)\n",
    "    # ----------------------------------------------------------------\n",
    "    def sample_motion(self, x_prev, e_prev, e_curr):\n",
    "        r\"\"\"\n",
    "        Samples each target's kinematic state \\(x_{k,n}\\) given\n",
    "        \\( x_{k-1,n} \\) and the existence variables \\( e_{k,n}, e_{k-1,n}\\).\n",
    "\n",
    "        From Eq. (7):\n",
    "        \\[\n",
    "          p(x_{k,n} \\mid x_{k-1,n}, e_{k,n}, e_{k-1,n}) = \n",
    "            \\begin{cases}\n",
    "              p_b(x_{k,n}) & \\text{if } \\{e_{k,n}=1, e_{k-1,n}=0\\}, \\\\\n",
    "              p_d(x_{k,n}) & \\text{if } e_{k,n}=0, \\\\\n",
    "              p_u(x_{k,n} \\mid x_{k-1,n}) & \\text{if } \\{e_{k,n}=1, e_{k-1,n}=1\\}.\n",
    "            \\end{cases}\n",
    "        \\]\n",
    "\n",
    "        where\n",
    "         - \\(p_b\\) is the birth density (Eq. (8)),\n",
    "         - \\(p_d\\) is the death density (Eq. (9)),\n",
    "         - \\(p_u\\) is the near-constant velocity update (Eq. (10)).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_prev : ndarray of shape (Nmax, 4)\n",
    "            States of the previous time step.\n",
    "        e_prev : ndarray of shape (Nmax,)\n",
    "            Existence vector at the previous time step.\n",
    "        e_curr : ndarray of shape (Nmax,)\n",
    "            Existence vector at the current time step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_curr : ndarray of shape (Nmax, 4)\n",
    "            Sampled states at the current time step.\n",
    "        \"\"\"\n",
    "        Nmax = len(e_prev)\n",
    "        x_curr = np.zeros((Nmax, 4), dtype=float)\n",
    "\n",
    "        for n in range(Nmax):\n",
    "            if (e_prev[n] == 0) and (e_curr[n] == 1):\n",
    "                # Birth => p_b(x_{k,n}), Eq. (8)\n",
    "                # Uniform in [0, Lx], [0, Ly], velocity in [-Vmax, Vmax].\n",
    "                x_birth = np.zeros(4, dtype=float)\n",
    "                x_birth[0] = np.random.uniform(0, self.Lx)               # x\n",
    "                x_birth[2] = np.random.uniform(0, self.Ly)               # y\n",
    "                x_birth[1] = np.random.uniform(-self.Vmax, self.Vmax)    # vx\n",
    "                x_birth[3] = np.random.uniform(-self.Vmax, self.Vmax)    # vy\n",
    "                x_curr[n, :] = x_birth\n",
    "\n",
    "            elif e_curr[n] == 0:\n",
    "                # Death => p_d(x_{k,n}) = δ(x_death), Eq. (9)\n",
    "                x_curr[n, :] = self.x_death\n",
    "\n",
    "            else:\n",
    "                # Update => p_u(x_{k,n} | x_{k-1,n}), Eq. (10)\n",
    "                # near-constant velocity: N(A x_{k-1,n}, Q).\n",
    "                mean = self.A @ x_prev[n, :]  # A is 4x4, x_prev[n,:] is 4x1\n",
    "                x_curr[n, :] = np.random.multivariate_normal(mean, self.Q)\n",
    "\n",
    "        return x_curr\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # LOG-LIKELIHOOD p(z_k | x_k), Eqs. (12)–(13)\n",
    "    # ----------------------------------------------------------------\n",
    "    def log_likelihood_measurements(self, z, x, e, LambdaC, LambdaX):\n",
    "        r\"\"\"\n",
    "        Computes \\(\\log p(z_k | x_k)\\) under the association-free model (Eqs. (12)–(13)):\n",
    "\n",
    "        \\[\n",
    "          p(z_k \\mid x_k) \\;=\\;\n",
    "           \\frac{\\exp(-\\mu_k)}{M_k!}\n",
    "           \\prod_{m=1}^{M_k} \\lambda(z_k^{(m)})  \\;,\\quad\n",
    "          \\text{where } \\mu_k = \\Lambda_C + \\sum_{n \\in \\text{active}} \\Lambda_X,\n",
    "        \\]\n",
    "        \\[\n",
    "          \\lambda(z) = \\Lambda_C\\, p_C(z) \\;+\\; \n",
    "                        \\sum_{n \\in \\text{active}} \\bigl[\\Lambda_X\\, p_x(z \\mid x_{k,n})\\bigr],\n",
    "        \\]\n",
    "        with\n",
    "         - \\(p_C(z)\\) uniform in \\([0,L_x]\\times [0,L_y]\\),\n",
    "         - \\(p_x(z \\mid x_{k,n}) = \\mathcal{N}(z; [x_{k,n}, y_{k,n}], \\text{CovMeas})\\).\n",
    "\n",
    "        We do everything in the log domain for numerical stability.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z : ndarray, shape (M_k, 2)\n",
    "            All measurements at time k.\n",
    "        x : ndarray, shape (Nmax, 4)\n",
    "            The states (x, vx, y, vy) for each target.\n",
    "        e : ndarray, shape (Nmax,)\n",
    "            Existence indicator for each target (0 or 1).\n",
    "        LambdaC : float\n",
    "            Mean clutter rate \\(\\Lambda_C\\).\n",
    "        LambdaX : float\n",
    "            Mean detection rate per active target \\(\\Lambda_X\\).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        log_like : float\n",
    "            The log of p(z_k | x_k).\n",
    "        \"\"\"\n",
    "        # Number of active targets\n",
    "        N_active = np.sum(e)\n",
    "        # mu_k = LambdaC + N_active * LambdaX\n",
    "        mu_k = LambdaC + N_active * LambdaX\n",
    "        # M_k = number of measurements\n",
    "        M_k = z.shape[0]\n",
    "\n",
    "        # log(M_k!)\n",
    "        if M_k <= 1:\n",
    "            log_factorial_M = 0.0\n",
    "        else:\n",
    "            log_factorial_M = np.sum(np.log(np.arange(1, M_k+1)))\n",
    "\n",
    "        # Start log p(z_k|x_k)\n",
    "        #  = - mu_k - log(M_k!) + sum_{m=1..M_k} [ log(lambda(z_m)) ]\n",
    "        log_like = -mu_k - log_factorial_M\n",
    "\n",
    "        # Uniform clutter density = 1 / (Lx * Ly)\n",
    "        clutter_density = 1.0 / (self.Lx * self.Ly)\n",
    "\n",
    "        # Utility for computing Gaussian pdf in 2D\n",
    "        def normal_pdf_2d(zm, mean_xy, Cov):\n",
    "            return multivariate_normal.pdf(zm, mean=mean_xy, cov=Cov)\n",
    "\n",
    "        for m in range(M_k):\n",
    "            zm = z[m, :]  # measurement\n",
    "            # lambda(z_m) = LambdaC * pC(z_m) + sum_{active n} [ LambdaX * px(z_m| x_{k,n}) ]\n",
    "            lam_val = LambdaC * clutter_density\n",
    "\n",
    "            for n in range(len(e)):\n",
    "                if e[n] == 1:\n",
    "                    # x[n,:] = (x, vx, y, vy)\n",
    "                    mean_xy = np.array([x[n, 0], x[n, 2]], dtype=float)\n",
    "                    lam_val += LambdaX * normal_pdf_2d(zm, mean_xy, self.CovMeas)\n",
    "\n",
    "            # If lam_val ~ 0 => log -> -inf\n",
    "            if lam_val < 1e-300:\n",
    "                return -1e16\n",
    "            log_like += np.log(lam_val)\n",
    "\n",
    "        return log_like\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Main Filtering Loop (MCMC-based Particle Algorithm)\n",
    "    # ---------------------------------------------------------\n",
    "    def filter(self, measurements, Nmax=5):\n",
    "        \"\"\"\n",
    "        Perform the MCMC-based particle filtering over a sequence of measurements\n",
    "        according to the approach in Section III-B and Algorithm 1 of the paper.\n",
    "\n",
    "        Steps:\n",
    "          1) For k=0..K-1:\n",
    "             (a) Prediction: sample e_k from e_{k-1} (Eq. (6)) & x_k from x_{k-1} (Eq. (7))\n",
    "             (b) Joint draw (Eq. (14)) => predicted particles\n",
    "             (c) Refinement: MCMC step that successively samples each target's {x_{k,n}, e_{k,n}}\n",
    "                 using q3(...) = p(x_{k,n} | x_{k-1,n}, e_{k,n}) p(e_{k,n} | e_{k-1,n}) (Eq. (15))\n",
    "             (d) Weight update with p(z_k | x_k) from Eqs. (12)–(13), in log form\n",
    "             (e) Resampling\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        all_particles : list of length K\n",
    "            all_particles[k] is the final particle set for time k.\n",
    "        card_mean : array of shape (K,)\n",
    "            Mean cardinality (number of alive targets) per time step.\n",
    "        card_std : array of shape (K,)\n",
    "            Standard deviation of the cardinality estimates.\n",
    "        \"\"\"\n",
    "        K = len(measurements)\n",
    "        # Initialize from (possibly) q1(...) (Eq. (14))\n",
    "        # Here we simply start all in a default \"dead\" state.\n",
    "        particles_prev = self.init_particles(Nmax=Nmax)\n",
    "\n",
    "        card_mean = np.zeros(K)\n",
    "        card_std = np.zeros(K)\n",
    "        all_particles = []\n",
    "\n",
    "        # Optional progress bar (from tqdm) for user convenience\n",
    "        for k in tqdm(range(K), desc=\"MCMC-PF Filtering\"):\n",
    "            z_k = measurements[k]\n",
    "\n",
    "            # ---- (a) & (b) Prediction Step & Joint Draw (Eq. (14)) ----\n",
    "            predicted = []\n",
    "            for i in range(self.Np):\n",
    "                x_old = particles_prev[i]['x']\n",
    "                e_old = particles_prev[i]['e']\n",
    "                w_old = particles_prev[i]['w']  # log-weight from previous\n",
    "\n",
    "                # Sample e_k from e_{k-1} (Eq. (6))\n",
    "                e_new = self.sample_existence(e_old)\n",
    "                # Sample x_k from x_{k-1}, e_k, e_{k-1} (Eq. (7))\n",
    "                x_new = self.sample_motion(x_old, e_old, e_new)\n",
    "\n",
    "                predicted.append({'x': x_new, 'e': e_new, 'w': w_old})\n",
    "\n",
    "            # ---- (c) Refinement Step (Eq. (15)) ----\n",
    "            # For each particle, run a short MCMC chain that updates each target's {x_{k,n}, e_{k,n}} successively.\n",
    "            # We skip re-drawing x_{k-1}, e_{k-1} (q2 not used in this simplified version).\n",
    "            for i in range(self.Np):\n",
    "                x_cur = predicted[i]['x'].copy()\n",
    "                e_cur = predicted[i]['e'].copy()\n",
    "\n",
    "                # Basic MCMC with Nburn + thinning * 1 (or more) steps\n",
    "                # Each iteration, we cycle through targets n=1..Nmax\n",
    "                for _ in range(self.Nburn + self.thinning):\n",
    "                    for n in range(Nmax):\n",
    "                        e_oldn = e_cur[n]\n",
    "                        x_oldn = x_cur[n, :].copy()\n",
    "\n",
    "                        # Propose a local change in existence or state:\n",
    "                        # The article uses q3(...) = p(x_k,n | x_{k-1,n}, e_k,n) p(e_k,n| e_{k-1,n}).\n",
    "                        # Here, we do a small random move if alive, or keep x_death if dead.\n",
    "                        if e_oldn == 1:\n",
    "                            # local small perturbation\n",
    "                            x_prop_n = x_oldn + 0.05 * randn(4)\n",
    "                        else:\n",
    "                            x_prop_n = self.x_death.copy()\n",
    "\n",
    "                        # Evaluate log-likelihood before\n",
    "                        ll_old = self.log_likelihood_measurements(z_k, x_cur, e_cur, self.LambdaC, self.LambdaX)\n",
    "\n",
    "                        # Modify just the nth target's state\n",
    "                        x_test = x_cur.copy()\n",
    "                        x_test[n, :] = x_prop_n\n",
    "                        # We could also consider flipping e_cur[n] with small probability\n",
    "                        # but typically that would require referencing e_{k-1,n}; omitted for brevity.\n",
    "\n",
    "                        ll_new = self.log_likelihood_measurements(z_k, x_test, e_cur, self.LambdaC, self.LambdaX)\n",
    "\n",
    "                        # Accept/reject (Metropolis-Hastings)\n",
    "                        alpha = np.exp(ll_new - ll_old)\n",
    "                        if np.random.rand() < alpha:\n",
    "                            x_cur[n, :] = x_prop_n\n",
    "\n",
    "                # After MCMC chain, compute final log-likelihood\n",
    "                w_final = self.log_likelihood_measurements(z_k, x_cur, e_cur, self.LambdaC, self.LambdaX)\n",
    "                predicted[i]['x'] = x_cur\n",
    "                predicted[i]['e'] = e_cur\n",
    "                predicted[i]['w'] = w_final\n",
    "\n",
    "            # ---- (d) Resampling ----\n",
    "            weights_log = np.array([p['w'] for p in predicted])\n",
    "            wmax = np.max(weights_log)\n",
    "            # Convert log-weights to normalized weights\n",
    "            weights_lin = np.exp(weights_log - wmax)\n",
    "            weights_lin /= np.sum(weights_lin)\n",
    "\n",
    "            indices = choice(self.Np, size=self.Np, replace=True, p=weights_lin)\n",
    "            new_particles = []\n",
    "            for idx in indices:\n",
    "                # Copy the chosen particle; set new log-weight=0 after resampling\n",
    "                new_particles.append({\n",
    "                    'x': predicted[idx]['x'].copy(),\n",
    "                    'e': predicted[idx]['e'].copy(),\n",
    "                    'w': 0.0\n",
    "                })\n",
    "\n",
    "            # ---- (e) Cardinality Estimation ----\n",
    "            card_est = [np.sum(p['e']) for p in new_particles]\n",
    "            card_mean[k] = np.mean(card_est)\n",
    "            card_std[k] = np.std(card_est)\n",
    "\n",
    "            # Move to next time step\n",
    "            particles_prev = new_particles\n",
    "            all_particles.append(new_particles)\n",
    "\n",
    "        return all_particles, card_mean, card_std\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4) HELPER FUNCTIONS FOR ESTIMATION AND PLOTTING\n",
    "# ----------------------------------------------------------------------\n",
    "def extract_average_particle_tracks(particle_set):\n",
    "    \"\"\"\n",
    "    Compute the weighted average of the particle states (x) and\n",
    "    the (fractional) existence (e) at a single time step.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    particle_set : list of dict\n",
    "        Each dict has keys:\n",
    "          'x': (Nmax, 4) - the kinematic state,\n",
    "          'e': (Nmax,) in {0,1} - existence indicators,\n",
    "          'w': float (the log-weight).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_avg : numpy array, shape=(Nmax, 4)\n",
    "        The weighted average of the kinematic states.\n",
    "    e_avg : numpy array, shape=(Nmax,)\n",
    "        The (fractional) weighted average of the existences, i.e. in [0,1].\n",
    "        (If you want hard 0/1, you can threshold this later.)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Gather log-weights in a NumPy array\n",
    "    log_w = np.array([p['w'] for p in particle_set], dtype=float)\n",
    "\n",
    "    # 2) Convert to linear weights safely (avoid exp underflow)\n",
    "    w_max = np.max(log_w)\n",
    "    w_lin = np.exp(log_w - w_max)\n",
    "\n",
    "    # 3) Normalize the weights\n",
    "    w_lin_sum = np.sum(w_lin)\n",
    "    if w_lin_sum < 1e-300:\n",
    "        # If all weights are extremely small or -inf, fallback:\n",
    "        w_lin = np.ones_like(w_lin) / len(w_lin)\n",
    "        w_lin_sum = 1.0\n",
    "    else:\n",
    "        w_lin /= w_lin_sum\n",
    "\n",
    "    # 4) Compute weighted average for x and e\n",
    "    #    We'll accumulate sums in arrays\n",
    "    #    x_sum will be shape (Nmax, 4), e_sum will be shape (Nmax,)\n",
    "    Nmax = particle_set[0]['x'].shape[0]\n",
    "    x_sum = np.zeros((Nmax, 4), dtype=float)\n",
    "    e_sum = np.zeros(Nmax, dtype=float)\n",
    "\n",
    "    for i, part in enumerate(particle_set):\n",
    "        x_sum += w_lin[i] * part['x']\n",
    "        e_sum += w_lin[i] * part['e']\n",
    "\n",
    "    x_avg = x_sum  # Weighted mean\n",
    "    e_avg = e_sum  # Weighted mean in [0,1]\n",
    "\n",
    "    return x_avg, e_avg\n",
    "\n",
    "def get_estimated_tracks_over_time(all_particles):\n",
    "    \"\"\"\n",
    "    Use the weighted average of the particles at each time k to produce\n",
    "    the estimated track over time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    all_particles : list of lists\n",
    "        all_particles[k] is the particle set at time k, \n",
    "        i.e. a list of dicts [{'x':..., 'e':..., 'w':...}, ...].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_est : numpy array, shape=(K, Nmax, 4)\n",
    "        Weighted-average states at each time k.\n",
    "    e_est : numpy array, shape=(K, Nmax)\n",
    "        Weighted-average existences (in [0,1]) at each time k.\n",
    "        (You can threshold later if you need strict 0/1.)\n",
    "    \"\"\"\n",
    "\n",
    "    K = len(all_particles)\n",
    "    Nmax = all_particles[0][0]['x'].shape[0]\n",
    "\n",
    "    x_est = np.zeros((K, Nmax, 4), dtype=float)\n",
    "    e_est = np.zeros((K, Nmax), dtype=float)\n",
    "\n",
    "    for k in range(K):\n",
    "        x_k, e_k = extract_average_particle_tracks(all_particles[k])\n",
    "        x_est[k, :, :] = x_k\n",
    "        e_est[k, :] = e_k\n",
    "\n",
    "    return x_est, e_est\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 5) MAIN SIMULATION AND ENHANCED PLOTTING\n",
    "# ----------------------------------------------------------------------\n",
    "def main_simulation(K=80, do_plot=True):\n",
    "    \"\"\"\n",
    "    Executes the complete simulation with enhanced plotting:\n",
    "     1) Generate ground truth for a maximum of 5 targets (births/deaths, near-constant velocity).\n",
    "     2) Generate measurements (association-free, Poisson-based).\n",
    "     3) Instantiate the MCMC-based Particle Filter with chosen parameters (Np, Nburn, etc.).\n",
    "     4) Perform filtering over the measurement sequence (Algorithm 1).\n",
    "     5) Extract weighted-average estimates over time.\n",
    "     6) Compute additional metrics like RMSE.\n",
    "     7) Optionally plot:\n",
    "        - True vs. estimated cardinality over time.\n",
    "        - True target trajectories.\n",
    "        - Estimated target trajectories.\n",
    "        - Existence probabilities over time.\n",
    "        - Measurements and estimates at selected time steps.\n",
    "    \"\"\"\n",
    "    # 1) Generate ground truth (Eq. (10), Section IV)\n",
    "    ground_truth = generate_ground_truth(K=K)\n",
    "\n",
    "    # 2) Generate measurements (Eqs. (12)–(13), association-free model)\n",
    "    measurements = generate_measurements(ground_truth)\n",
    "\n",
    "    # 3) Instantiate MCMC-based Particle Filter with smaller defaults\n",
    "    #    (the article used Np=4000, Nburn=1000, etc. for final results,\n",
    "    #     but here we reduce them for a quicker demo).\n",
    "    pf = MCMCParticleFilter(Np=100,    # fewer particles for demo\n",
    "                            Nburn=2,  # fewer burn-in steps\n",
    "                            thinning=1,\n",
    "                            PB=0.1,\n",
    "                            PD=0.1)\n",
    "\n",
    "    # 4) Run the particle filter\n",
    "    all_particles, card_mean, card_std = pf.filter(measurements, Nmax=5)\n",
    "\n",
    "    # 5) Extract track estimates (weighted-average tracks) over time\n",
    "    x_est, e_est = get_estimated_tracks_over_time(all_particles)\n",
    "\n",
    "    # 6) (Optional) Plot results\n",
    "    if do_plot:\n",
    "        import matplotlib.pyplot as plt\n",
    "        e_true = ground_truth['e_true']  # shape (K, Nmax)\n",
    "        x_true = ground_truth['x_true']  # shape (K, Nmax, 4)\n",
    "        true_card = np.sum(e_true, axis=1)\n",
    "\n",
    "        # (A) Plot the cardinality over time\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(true_card, 'k-', label='True Cardinality')\n",
    "        plt.plot(card_mean, 'r--', label='Estimated Mean Cardinality')\n",
    "        plt.fill_between(\n",
    "            range(K),\n",
    "            card_mean - card_std,\n",
    "            card_mean + card_std,\n",
    "            color='r',\n",
    "            alpha=0.2,\n",
    "            label='±1 std'\n",
    "        )\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Cardinality')\n",
    "        plt.title('Cardinality: True vs. Estimated Over Time')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # (B) Plot the true trajectories in x-y plane\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        for n in range(e_true.shape[1]):\n",
    "            alive_times = np.where(e_true[:, n] == 1)[0]\n",
    "            if len(alive_times) > 0:\n",
    "                # Extract x and y positions\n",
    "                xvals = x_true[alive_times, n, 0]\n",
    "                yvals = x_true[alive_times, n, 2]\n",
    "\n",
    "                # Plot true trajectory\n",
    "                plt.plot(xvals, yvals, '-o', label=f\"True T{n+1}\")\n",
    "\n",
    "                # Mark birth and death\n",
    "                k_start = alive_times[0]\n",
    "                k_stop = alive_times[-1]\n",
    "\n",
    "                plt.plot(x_true[k_start, n, 0],\n",
    "                         x_true[k_start, n, 2],\n",
    "                         'ko',  # Black circle for birth\n",
    "                         markersize=8)\n",
    "                if k_stop < (K - 1):\n",
    "                    plt.plot(x_true[k_stop, n, 0],\n",
    "                             x_true[k_stop, n, 2],\n",
    "                             'k^',  # Black triangle for death\n",
    "                             markersize=8)\n",
    "\n",
    "        plt.xlabel('x [m]')\n",
    "        plt.ylabel('y [m]')\n",
    "        plt.xlim([0, 5000])\n",
    "        plt.ylim([0, 5000])\n",
    "        plt.title('True Target Trajectories in x-y Plane')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # (C) Plot the estimated trajectories in x-y plane\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        for n in range(e_est.shape[1]):\n",
    "            alive_times_est = np.where(e_est[:, n] > 0.5)[0]\n",
    "            if len(alive_times_est) > 0:\n",
    "                x_est_vals = x_est[alive_times_est, n, 0]\n",
    "                y_est_vals = x_est[alive_times_est, n, 2]\n",
    "\n",
    "                # Plot estimated trajectory\n",
    "                plt.plot(x_est_vals, y_est_vals, '--x', label=f\"Estimated T{n+1}\")\n",
    "\n",
    "                # Mark birth and death estimates\n",
    "                k_start_est = alive_times_est[0]\n",
    "                k_stop_est = alive_times_est[-1]\n",
    "\n",
    "                plt.plot(x_est[k_start_est, n, 0],\n",
    "                         x_est[k_start_est, n, 2],\n",
    "                         'rx',  # Red X for birth\n",
    "                         markersize=8)\n",
    "                if k_stop_est < K:\n",
    "                    plt.plot(x_est[k_stop_est, n, 0],\n",
    "                             x_est[k_stop_est, n, 2],\n",
    "                             'r^',  # Red triangle for death\n",
    "                             markersize=8)\n",
    "\n",
    "        plt.xlabel('x [m]')\n",
    "        plt.ylabel('y [m]')\n",
    "        plt.xlim([0, 5000])\n",
    "        plt.ylim([0, 5000])\n",
    "        plt.title('Estimated Target Trajectories in x-y Plane')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # (D) Compute and plot RMSE\n",
    "        rmse = compute_rmse(x_true, x_est, e_true, e_est)\n",
    "        print(f'RMSE over all time steps and targets: {rmse:.2f} meters')\n",
    "\n",
    "        # (E) Plot existence probabilities over time\n",
    "        plot_existence_probabilities(e_true, e_est, K, Nmax)\n",
    "\n",
    "        # (F) Plot measurements and estimates at selected time steps\n",
    "        selected_time_steps = [24, 25, 50, 51, 75]\n",
    "        plot_measurements_and_estimates(measurements, x_true, x_est, e_true, e_est, selected_time_steps, K, Nmax)\n",
    "\n",
    "    return ground_truth, measurements, (card_mean, card_std), x_est, e_est\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 6) HELPER FUNCTIONS FOR ENHANCED PLOTTING\n",
    "# ----------------------------------------------------------------------\n",
    "def compute_rmse(x_true, x_est, e_true, e_est):\n",
    "    \"\"\"\n",
    "    Compute the Root Mean Square Error (RMSE) between true and estimated positions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_true : ndarray, shape=(K, Nmax, 4)\n",
    "        True states.\n",
    "    x_est : ndarray, shape=(K, Nmax, 4)\n",
    "        Estimated states.\n",
    "    e_true : ndarray, shape=(K, Nmax)\n",
    "        True existence indicators.\n",
    "    e_est : ndarray, shape=(K, Nmax)\n",
    "        Estimated existence indicators.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rmse : float\n",
    "        Root Mean Square Error.\n",
    "    \"\"\"\n",
    "    total_error = 0.0\n",
    "    count = 0\n",
    "    for k in range(x_true.shape[0]):\n",
    "        for n in range(x_true.shape[1]):\n",
    "            if e_true[k, n] == 1 and e_est[k, n] > 0.5:\n",
    "                true_pos = x_true[k, n, [0, 2]]\n",
    "                est_pos = x_est[k, n, [0, 2]]\n",
    "                error = np.linalg.norm(true_pos - est_pos)\n",
    "                total_error += error ** 2\n",
    "                count += 1\n",
    "    rmse = np.sqrt(total_error / count) if count > 0 else None\n",
    "    return rmse\n",
    "\n",
    "def plot_existence_probabilities(e_true, e_est, K, Nmax):\n",
    "    \"\"\"\n",
    "    Plot the existence probabilities of each target over time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    e_true : ndarray, shape=(K, Nmax)\n",
    "        True existence indicators.\n",
    "    e_est : ndarray, shape=(K, Nmax)\n",
    "        Estimated existence probabilities.\n",
    "    K : int\n",
    "        Number of timesteps.\n",
    "    Nmax : int\n",
    "        Number of targets.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for n in range(Nmax):\n",
    "        plt.plot(range(K), e_est[:, n], label=f'Target {n+1} Existence Probability')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Existence Probability')\n",
    "    plt.title('Existence Probabilities Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_measurements_and_estimates(measurements, x_true, x_est, e_true, e_est, selected_time_steps, K, Nmax):\n",
    "    \"\"\"\n",
    "    Plot measurements and estimates at selected time steps.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    measurements : list of ndarray\n",
    "        List of measurements at each time step.\n",
    "    x_true : ndarray, shape=(K, Nmax, 4)\n",
    "        True states.\n",
    "    x_est : ndarray, shape=(K, Nmax, 4)\n",
    "        Estimated states.\n",
    "    e_true : ndarray, shape=(K, Nmax)\n",
    "        True existence indicators.\n",
    "    e_est : ndarray, shape=(K, Nmax)\n",
    "        Estimated existence indicators.\n",
    "    selected_time_steps : list of int\n",
    "        Time steps to plot.\n",
    "    K : int\n",
    "        Total number of time steps.\n",
    "    Nmax : int\n",
    "        Number of targets.\n",
    "    \"\"\"\n",
    "    for k in selected_time_steps:\n",
    "        if k >= K:\n",
    "            print(f\"Time step {k} is out of range. Skipping.\")\n",
    "            continue\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.xlim(0, 5000)\n",
    "        plt.ylim(0, 5000)\n",
    "        plt.xlabel('x [m]')\n",
    "        plt.ylabel('y [m]')\n",
    "        plt.title(f'Measurements and Estimates at Time Step {k+1}')\n",
    "\n",
    "        # Plot measurements\n",
    "        z_k = measurements[k]\n",
    "        if len(z_k) > 0:\n",
    "            plt.scatter(z_k[:, 0], z_k[:, 1], c='k', marker='.', label='Measurements')\n",
    "\n",
    "        # Plot true target positions\n",
    "        for n in range(Nmax):\n",
    "            if e_true[k, n] == 1:\n",
    "                plt.scatter(x_true[k, n, 0], x_true[k, n, 2],\n",
    "                            marker='o', edgecolors='b', facecolors='none', label=f'True T{n+1}' if k == selected_time_steps[0] else \"\")\n",
    "\n",
    "        # Plot estimated target positions\n",
    "        for n in range(Nmax):\n",
    "            if e_est[k, n] > 0.5:\n",
    "                plt.scatter(x_est[k, n, 0], x_est[k, n, 2],\n",
    "                            marker='x', color='r', label=f'Est T{n+1}' if k == selected_time_steps[0] else \"\")\n",
    "\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 7) RUN EXAMPLE\n",
    "# ----------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: single run, T=100\n",
    "    T = 100\n",
    "    gt, meas, (c_mean, c_std), x_estimated, e_estimated = main_simulation(K=T, do_plot=True)\n",
    "\n",
    "    # The user can adapt the code to do multiple runs or compute advanced metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
